<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.1//EN" "http://www.w3.org/TR/xhtml11/DTD/xhtml11.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en">
	<head>
		<title>Seenivasan Lalithkumar&#39;s Homepage</title>
		<link rel="shortcut icon" href="pic/lalith-icon.png">
		<meta http-equiv="Content-Type" content="text/html;charset=utf-8" />
		<meta name="description" content="Seenivasan Lalithkumar&#39;s home page">
		<meta name="keywords" content="Seenivasan Lalithkumar, Lalithkumar, Seenivasan, lalith, researcher, medical AI" />
		<meta name="google-site-verification" content="oxvLOLkrGhuP5j1k3XZxSqzG_gz2jVlLImAT0WfH4co" />
		<!-- <meta name="viewport" content="width=900px, initial-scale=1"> -->
		<link rel="stylesheet" href="jemdoc.css" type="text/css">
		<script src="conference.js" type="text/javascript"></script>

	</head>
	<body id="namecard">
		<div id="namecardtop">
		</div>
		<div id="namecardbottom">
			<div id="namecardbottom">
				<div id="namecardimg"><img src="./pic/lalith2.png"  class="namecard"></div>
				<div id="namecardtext">
					<h1>
						Seenivasan <u>Lalith</u>kumar
					</h1>
					<p>
						<b>PostDoctoral Research Fellow</b><br>
						<a href="https://arcade.cs.jhu.edu/">ARCADE Lab</a> <br>
						<a href="https://www.cs.jhu.edu/"> Department of Computer Science</a><br>
						<a href="https://www.jhu.edu/">Johns Hopkins University</a><br>
						<a href="mailto:lseeniv1@jh.edu"><img src="./pic/mail.png" height="20px" style="margin-bottom:-5px; border-radius: 10px; border: 2px solid #16213e;"> &nbsp Lalithkumar_@u.nus.edu</a>
					</p>	
					<p> 
						<a href="https://scholar.google.com/citations?user=btxQyh8AAAAJ&hl=en&oi=ao"><img src="./pic/google_scholar.png" class="icon"></a> &ensp;
						<a href="https://www.researchgate.net/profile/Lalithkumar-Seenivasan"><img src="./pic/researchgate.png" class="icon"></a> &ensp;
						<a href="https://orcid.org/0000-0002-0103-1234"><img src="./pic/orcid.jpeg" class="icon"></a> &ensp;
						<a href="https://github.com/lalithjets"><img src="./pic/github.png" class="icon"></a> &ensp;
						<a href="https://www.linkedin.com/in/lalithkumar-seenivasan/"><img src="./pic/LinkedIn_s.png" class="icon"></a> &ensp;
						
					</p>
				</div>
			</div>
		</div>
	</body>
	
	<body id="content">
		
		<div class="buttontab">
				<button class="button" onclick="openTab(event, 'Highlights')" id="defaultOpen">Highlights</button> &nbsp 
				<button class="button" onclick="openTab(event, 'Publications')">Publications (Full List)</button> &nbsp
				<button class="button" onclick="openTab(event, 'Robotic-Projects')">Robotics Projects</button> &nbsp
				<button class="button" onclick="openTab(event, 'Resume')">Resume</button>
		</div>
		
		<div id="Highlights" class="tabscontent">
			<div id="biography" class="subheading">
				<h2 ><img src="./pic/biography.png" class = "hicon"> <u>Short Biography</u></h2>

				<p>
					I am a Postdoctoral Research Fellow at the <a href="https://arcade.cs.jhu.edu/">Advanced Robotics and Computationally AugmenteD Environments (ARCADE) Lab</a>,
					led by <a href="https://arcade.cs.jhu.edu/members/mathias-unberath.html">Dr. Mathias Unberath</a>, 
					at <a href="https://www.cs.jhu.edu/"> Department of Computer Science</a>, <a href="https://www.jhu.edu/">Johns Hopkins University</a>. 
					My research aims to develop trailblazing assistive and automation technologies that transform healthcare. 
					I particularly focus on transforming healthcare across three complementary domains: (a) clinical training, 
					(b) surgical assistance and automation, and (c) clinical workflow analysis and optimization.

				</p>

				<p>
					I earned my Ph.D. in <a href="https://cde.nus.edu.sg/bme/"> Biomedical Engineering</a> from the <a href="https://www.nus.edu.sg/">National University of Singapore (NUS)</a>, 
					under the mentorship of <a href="http://www.labren.org/mm/principal-investigator/">Dr. Ren Hongliang</a>. 
					My doctoral thesis, titled “Scene Understanding through Multimodal Reasoning for Robotic Surgery”, focused on pioneering and 
					advancing vision-language models for surgical scene understanding tasks, including surgical visual-question answering 
					(<a href="https://arxiv.org/pdf/2304.09974"> SurgicalGPT</a>  & <a href="https://arxiv.org/pdf/2206.11053"> Surgical-VQA</a>) 
					and <a href="https://arxiv.org/pdf/2201.11957"> surgical scene graphs</a>. My work on SurgicalGPT won the MICCAI 2023 STudent Author Registration (STAR) Award. 
					I also hold a Bachelor's degree (with Honours) in <a href="https://cde.nus.edu.sg/ece/">Electrical Engineering</a> from the <a href="https://www.nus.edu.sg/">NUS</a> and
					a Diploma in <a href="https://www.tp.edu.sg/t66">Mechatronics</a> from <a href="https://www.tp.edu.sg/">Temasek Polytechnic</a>, Singapore. 
					During my bachelor's studies, I was awarded the NUS Outstanding Undergraduate Researcher Prize and
					the Faculty of Engineering's 32nd Innovation & Research Award for my work in the Design-Centric Programme, 
					where our multi-disciplinary team successfully miniaturized surgical tools for minimally invasive surgery using 
				 	a novel tendon-driven actuation mechanism.
				</p>

				<p>
					Beyond academia, I bring over four years of industry experience at a robotics startup, 
					where I progressed from a Software Engineer to Engineering Manager, leading the development of delivery robots for the food and warehouse sector. 
					I also have an extensive leadership background, including close to a decade of service as a senior police officer in the Singapore 
					Police Force as part of Singapore's mandatory national service.

				</p>

				<p>
					I have authored numerous peer-reviewed journal articles and conference papers, including MICCAI, ICRA, RA-L, and IJCARS. 
					I have also won multiple academic and leadership awards. For a detailed list, please refer to my CV or Google Scholar.
				</p>
			</div>

			<div id="newfeeds" class="subheading">
				<h2><img src="./pic/news_feeds.png" class="hicon"/> <u>News Feeds</u></h2>
				
				<div class="scrolling-wrapper">

					<div class="card">
						<h2>New Position</h2> <h3>[02/2024] </h3>
						<p> Joined <a href="https://arcade.cs.jhu.edu/" target="_blank"> ARCADE Lab</a> as PostDoctoral Research Fellow.</p> 
					</div>

					<div class="card">
						<h2>Completed PhD</h2> <h3>[01/2024] </h3>
						<p> Successfully defended my Thesis and completed my PhD.</p> 
					</div>
					
					<div class="card">
						<h2>Conf & Journal Paper</h2> <h3>[06/2023] </h3>
						<p> A paper on <a href="https://arxiv.org/abs/2304.09974"
							 target="_blank">SurgicalGPT: End-to-End Language-Vision GPT for Visual Question Answering in Surgery</a> was accepted in   
							 <a href="https://conferences.miccai.org/2023/en/" target="_blank"> MICCAI 2023</a> Conference.</p> 
					</div>
					
					<div class="card">
						<h2>Conf WS Presentation</h2><h3>[06/2023]</h3>
						<p> Presented a long abstract on <a href="https://drive.google.com/file/d/1kYx_XHiS5NvVFThOcVjXyhgVPpP1OEtK/view?usp=sharing" target="_blank">Relational Reasoning in VQA Models for Visual
							Question Answering in Robotic Surgery</a> accepted in <a href="https://sites.google.com/view/icra2023workshop-surgicalrobot/?pli=1" target="_blank">ICRA 2023 Workshop: Surgical Robotics</a>.</p>
					</div>

					<div class="card">
						<h2>Conf Presentation</h2><h3>[05/2023]</h3>
						<p> Presented 2 papers [<a href="https://discovery.ucl.ac.uk/id/eprint/10159683/1/Rethinking_Feature_Extraction_Gradient-based_Localized_Feature_Extraction_for_End-to-End_Surgical_Downstream_Tasks.pdf"
							 target="_blank">Gradient-Based Localized Feature Extraction for End-To-End Surgical Downstream Tasks</a> and <a href="https://arxiv.org/pdf/2305.11692v1.pdf" target="_blank">Surgical-VQLA</a>]
							accepted in <a href="https://www.icra2023.org/" target="_blank">ICRA 2023 Conference</a>.</p>
					</div>

					<div class="card">
						<h2>Conference Reviewer</h2><h3>[04/2023 - 05/2023]</h3>
						<p> Served as a reviewer for 9 papers in <a href="https://conferences.miccai.org/2023/en/" target="_blank"> MICCAI 2023</a> Conference.</p>
					</div>
					
					<div class="card">
						<h2> Book Chapter </h2><h3> [02/2023] </h3>
						<p> A book chapter on <a href="https://link.springer.com/chapter/10.1007/978-981-19-5932-5_13" target="_blank">Untethered 
							Soft Ferromagnetic Quad-Jaws Cootie Catcher with Selectively Coupled Degrees of Freedom </a> was accepted.</p>
					</div>
					<div class="card">
						<h2> Journal Paper </h2><h3> [02/2023] </h3>
						<p> A paper on <a href="https://arxiv.org/pdf/2302.01049.pdf" target="_blank">Paced-Curriculum Distillation with Prediction 
							and Label Uncertainty for Image Segmentation</a> accepted in International Journal of Computer Assisted Radiology and Surgery.</p>
					</div>

					<div class="card">
						<h2>Conference Paper</h2><h3>[01/2023]</h3>
						<p> A paper on <a href="https://arxiv.org/pdf/2305.11692v1.pdf" target="_blank">Surgical-VQLA: Transformer with Gated Vision-Language Embedding for 
							Visual Question Localized-Answering in Robotic Surgery</a> was accepted in <a href="https://www.icra2023.org/" 
							target="_blank">IEEE ICRA 2023 Conference</a>. </p>
					</div>

					<div class="card">
						<h2>Conf & Journal Paper</h2> <h3>[10/2022] </h3>
						<p> A paper on <a href="https://discovery.ucl.ac.uk/id/eprint/10159683/1/Rethinking_Feature_Extraction_Gradient-based_Localized_Feature_Extraction_for_End-to-End_Surgical_Downstream_Tasks.pdf"
							 target="_blank">Rethinking Feature Extraction: Gradient-Based Localized Feature Extraction for End-To-End Surgical Downstream Tasks</a> was accepted in both IEEE RA-L Journal 
							 and  <a href="https://www.icra2023.org/" target="_blank">IEEE ICRA 2023 Conference</a>. </p> 
					</div>

					<div class="card">
						<h2>Competition</h2><h3>[09/2022]</h3>
						<p> Participated in <a href="https://www.synapse.org/#!Synapse:syn28548633/wiki/" target="_blank">SimCol-to-3D 2022 - 3D Reconstruction During Colonoscopy</a> in 
							<a href="https://conferences.miccai.org/2022/en/" target="_blank">MICCAI 2022 Conference</a>.</p> 
					</div> 

					<div class="card">
						<h2>Conf Presentation</h2><h3>[09/2022]</h3>
						<p> Presenting my paper on <a href="https://arxiv.org/pdf/2206.11053.pdf" target="_blank">Surgical-VQA: Visual Question Answering in Surgical Scenes Using Transformer</a>
							accepted in <a href="https://conferences.miccai.org/2022/en/" target="_blank">MICCAI 2022 Conference</a>.</p>
					</div> 

					<div class="card">
						<h2>Conference Paper</h2><h3>[06/2022]</h3>
						<p> A paper on <a href="https://arxiv.org/pdf/2206.11053.pdf" target="_blank">Surgical-VQA: Visual Question Answering in Surgical Scenes Using Transformer</a> was accepted 
							in <a href="https://conferences.miccai.org/2022/en/" target="_blank">MICCAI 2022 Conference</a>. </p> 
					</div>
						
					<div class="card">
						<h2>Conf Presentation</h2> <h3>[05/2022]</h3>
						<p> Presented my paper on on <a href="https://arxiv.org/pdf/2201.11957.pdf" target="_blank">Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding</a> at
							 <a href="https://www.icra2022.org/" target="_blank">IEEE ICRA 2022 Conference</a>.</p> 
					</div> 

					<div class="card">
						<h2>Journal Paper</h2><h3>[05/2022]</h3> 
						<p> A paper on <a href="https://www.mdpi.com/2313-7673/7/2/68" target="_blank">Biomimetic Incremental Domain Generalization with a Graph Network for Surgical Scene 
							Understanding</a> was accepted in MDPI Biomimetics Journal. </p>
					</div>

					<div class="card">
						<h2>Conf & Journal Paper</h2><h3>[01/2022]</h3>
						<p> A paper on <a href="https://arxiv.org/pdf/2201.11957.pdf" target="_blank">Global-Reasoned Multi-Task Learning Model for Surgical Scene Understanding</a> was accepted 
							in both IEEE RA-L Journal and  <a href="https://www.icra2022.org/" target="_blank">IEEE ICRA 2022 Conference</a>. </p>
					</div>

					<div class="card">
						<h2>Competition</h2><h3>[10/2021]</h3> 
						<p> Participated in <a href="https://arxiv.org/pdf/2202.05821.pdf" target="_blank">PEg TRAnsfer Workflow Recognition Challenge Report: Do Multi-Modal Data 
							Improve Recognition?</a> in <a href="https://www.miccai2021.org/en/" target="_blank">MICCAI 2021 Conference</a>. </p>
					</div>

					<div class="card">
						<h2>Competition</h2><h3>[10/2021]</h3>
						<p> Participated in <a href="https://arxiv.org/pdf/2204.04746.pdf" target="_blank">CholecTriplet2021: A benchmark challenge for surgical action triplet 
							recognition</a> in <a href="https://www.miccai2021.org/en/" target="_blank">MICCAI 2021 Conference</a>. </p>
					</div>

					<div class="card">
						<h2>Conference Workshop</h2><h3>[09/2021]</h3>
						<p> A paper on <a href="https://arxiv.org/pdf/2109.05263" target="_blank">Class-Distribution-Aware Calibration for Long-Tailed 
							Visual Recognition</a> is accepted in <a href="https://sites.google.com/view/udlworkshop2021/home?pli=1" target="_blank">UDL Workshop, ICML 2021 Conference</a>. </p>
					</div>

					<div class="card">
						<h2>Conference Paper</h2><h3>[08/2021]</h3> 
						<p> A paper on <a href="https://www.researchgate.net/profile/Liang-Qiu-7/publication/354075045_ScoopNet_6DOF_Pose_Estimation_pipeline_for_Origami-inspired_Worm_Robots/links/62ac796923f3283e3aef5d29/ScoopNet-6DOF-Pose-Estimation-pipeline-for-Origami-inspired-Worm-Robots.pdf" 
							target="_blank">ScoopNet: 6DOF Pose Estimation pipeline for Origami-inspired Worm Robots</a> was accepted in <a href="https://www.ieee-ras.org/component/rseventspro/event/2009-icdl-2021" target="_blank"> 
							IEEE ICDL 2021 Conference</a>. </p>
					</div>

					<div class="card">
						<h2>Journal Paper</h2><h3>[03/2021]</h3>
						<p>A paper on <a href="https://ieeexplore.ieee.org/abstract/document/9262941" target="_blank">Shape Tracking of Flexible Morphing Matters 
							From Depth Images</a> was accepted in IEEE Sensors Journal. </p> 
					</div>

					<div class="card">
						<h2>Journal Paper</h2>
						<h3>[07/2020]</h3>
						<p> A paper on <a href="https://onlinelibrary.wiley.com/doi/full/10.1002/aisy.202000092" target="_blank">Stent Deployment Detection Using Radio Frequency-Based Sensor and 
							Convolutional Neural Networks</a> was accepted in Advanced Intelligent Systems Journal. </p>
					</div>
						
					<div class="card">
						<h2>Book Chapter</h2><h3>[07/2020]</h3> 
						<p>A book chapter on <a href="https://www.sciencedirect.com/science/article/pii/B9780128213506000123" target="_blank">Tunable stiffness using negative Poisson's ratio toward 
							load-bearing continuum tubular mechanisms in medical robotics</a> was accepted.</p>
					</div>

					<div class="card">
						<h2>Book Chapter</h2><h3>[06/2020]</h3> 
						<p>A book chapter on <a href="https://www.sciencedirect.com/science/article/pii/B9780128175958000079" target="_blank">Tendon routing and anchoring for cable-driven single-port 
							surgical manipulators with spring backbones and luminal constraints</a> was accepted. </p>
					</div>
						
					<div class="card">
						<h2>Conference Paper</h2><h3>[06/2020]</h3> 
						<p>A paper on <a href="https://arxiv.org/abs/2007.03357" target="_blank">Graph Structure Representation in Robotic Surgery</a> was accepted 
							in <a href="https://miccai2020.org/en/" target="_blank">MICCAI 2020 Conference</a>.</p>
					</div>
						
					<div class="card">
						<h2>Start of PhD</h2><h3>[08/2019]</h3>
						<p> Started my PhD under Prof. Ren Hongliang at National University of Singapore.</p>
					</div>
				</div>
			</div>

			<div id="Key-Publications" class="subheading">
				<h2><img src="./pic/manuscript.png" class="hicon"/> <u>Key Publications</u></h2>
				<p style="text-align: center;">(* Co-first author)</p>

				<div class="keypublicationtiles keypublicationtile">
					<img src="./paper_photos/SurgicalGPT.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							 
							<b>SurgicalGPT: End-to-End <br> Language-Vision GPT for Visual  <br> Question Answering in Surgery.</b> <br>
							<b>Lalithkumar Seenivasan*</b>, Mobarakol Islam*, <br> Gokul Kannan and Hongliang Ren. <br>
							<em>Medical Image Computing and Computer-Assisted <br> Intervention</em> (<i><b>MICCAI</b></i>), 2023.
							<br>
							[<a href="https://arxiv.org/abs/2304.09974" target="_blank">preprint</a>]
							[<a href="https://github.com/lalithjets/" target="_blank">code (will be made available soon)</a>]
							<!-- [<a href="https://drive.google.com/file/d/19RYYjkokXWv5j_Wayjkc6bEKjSXdzaNB/view?usp=sharing" target="_blank">poster</a>]  -->
							<!-- [<a href="https://youtu.be/4Db8NSEW-FY" target="_blank">video</a>]  -->
						</p>
					</div>
				</div>
				
				<div class="keypublicationtiles keypublicationtile">
					<img src="./paper_photos/Surgical_VQA.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b>Surgical-VQA: Visual Question <br> Answering in Surgical Scenes <br> Using Transformer.</b> <br>
							<b>Lalithkumar Seenivasan*</b>, Mobarakol Islam*, <br> Adithya K. Krishna and Hongliang Ren. <br>
							<em>Medical Image Computing and Computer-Assisted <br> Intervention</em> (<i><b>MICCAI</b></i>), 2022.
							<br>
							[<a href="https://arxiv.org/pdf/2206.11053.pdf" target="_blank">preprint</a>]
							[<a href="https://github.com/lalithjets/Surgical_VQA" target="_blank">code</a>]
							[<a href="https://drive.google.com/file/d/19RYYjkokXWv5j_Wayjkc6bEKjSXdzaNB/view?usp=sharing" target="_blank">poster</a>] 
							[<a href="https://youtu.be/4Db8NSEW-FY" target="_blank">video</a>] 
						</p>
					</div>
				</div>
				
				<div class="keypublicationtiles keypublicationtile">
					<img src="./paper_photos/Global-Reasoned_Multi-Task_Learning.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b>Global-Reasoned Multi-Task <br> Learning Model for Surgical <br> Scene Understanding. </b><br>
							<b>Lalithkumar Seenivasan*</b>, Sai Mitheran*, <br> Mobarakol Islam and Hongliang Ren. <br>
							<em>IEEE Robotics and Automation Letters & IEEE International <br> Conference on Robotics and Automation</em> (<i><b>RA-L & ICRA</b></i>), 2022.
							<br>
							[<a href="https://arxiv.org/pdf/2201.11957.pdf" target="_blank">preprint</a>][<a href="https://github.com/lalithjets/Global-reasoned-multi-task-model" target="_blank">code</a>]
							[<a href="https://drive.google.com/file/d/1deAY5ThZRm9Y3AFX3ArIvde-LdE8imfx/view?usp=sharing" target="_blank">poster</a>]
							[<a href="https://youtu.be/UOIcp3y4o1U" target="_blank">video</a>] 
						</p>
					</div>
				</div>

				
				  
				<div class="keypublicationtiles keypublicationtile">
					<img src="./paper_photos/Rethinking Feature Extraction.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b>Rethinking Feature Extraction: Gradient-Based <br> Localized Feature Extraction for <br> End-To-End Surgical Downstream Tasks.</b> <br>
							Winnie Pang*, Mobarakol Islam*, Sai Mitheran, <b>Lalithkumar <br> Seenivasan</b>, Mengya Xu and Hongliang Ren. <br>
							<em>IEEE Robotics and Automation Letters & IEEE International <br> Conference on Robotics and Automation</em> (<i><b>RA-L & ICRA</b></i>), 2023.
							<br>
							[<a href="https://discovery.ucl.ac.uk/id/eprint/10159683/1/Rethinking_Feature_Extraction_Gradient-based_Localized_Feature_Extraction_for_End-to-End_Surgical_Downstream_Tasks.pdf" target="_blank">preprint</a>]
							[<a href="https://github.com/PangWinnie0219/GradCAMDownstreamTask" target="_blank">code</a>]
							[<a href="https://youtu.be/wPvV8IvS2tE" target="_blank">video</a>] 
						</p>
					</div>
				</div>

				
				<div class="keypublicationtiles keypublicationtile">
					<img src="paper_photos/Learning_Reasoning_Surgical_Graph.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b>Learning and Reasoning with <br> the Graph Structure Representation <br> in Robotic Surgery. </b><br>
							Mobarakol Islam, <b>Lalithkumar Seenivasan</b>, <br> Lim Chwee Ming and Hongliang Ren. <br>
							<em>Medical Image Computing and Computer Assisted <br> Intervention</em> (<i><b>MICCAI</b></i>), 2020. 
							<br> 
							[<a href="https://arxiv.org/pdf/2007.03357.pdf" target="_blank">preprint</a>][<a href="https://github.com/mobarakol/Surgical_SceneGraph_Generation" target="_blank">code</a>] 
						</p>
					</div>
				</div>


				<div class="keypublicationtiles keypublicationtile">
					<img src="./paper_photos/Surgical_VQLA.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b>Surgical-VQLA: Transformer with Gated <br> Vision-Language Embedding for Visual Question <br> Localized-Answering in Robotic Surgery.</b> <br>
							Long Bai*, Mobarakol Islam*, <br> <b>Lalithkumar Seenivasan</b> and Hongliang Ren. <br>
							<em>IEEE International Conference on Robotics and Automation <br> </em> (<i><b>ICRA</b></i>), 2023.
							<br>
							[<a href="https://arxiv.org/pdf/2305.11692v1.pdf" target="_blank">preprint</a>]
							[<a href="https://github.com/longbai1006/surgical-vqla" target="_blank">code</a>]
							<!-- [<a href="https://drive.google.com/file/d/19RYYjkokXWv5j_Wayjkc6bEKjSXdzaNB/view?usp=sharing" target="_blank">poster</a>]  -->
							<!-- [<a href="https://youtu.be/4Db8NSEW-FY" target="_blank">video</a>]  -->
						</p>
					</div>
				</div>

				
				<!-- <div class="keypublicationtiles keypublicationtile">
					<img src="./paper_photos/TA_MTL.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b>Task-aware asynchronous multi-task model with class <br> incremental contrastive learning for surgical <br> scene understanding. </b><br>
							<b>Lalithkumar Seenivasan</b>, Mobarakol Islam, Mengya Xu, <br> Chwee Ming Lim and Hongliang Ren. <br>
							<em>International Journal of Computer Assisted Radiology and <br> Surgery</em> (<i><b>IJCARS</b></i>), 2023.
							<br>
							[<a href="https://arxiv.org/pdf/2211.15327.pdf" target="_blank">preprint</a>]
							[<a href="https://github.com/lalithjets/Domain-adaptation-in-MTL" target="_blank">code</a>]
						</p>
					</div>
				</div> -->

				<!-- <div class="keypublicationtiles keypublicationtile">
					<img src="paper_photos/pcd.png" class="key-pub-title"/>
					<br>
					<div id="key-pub-tile-caption">
						<p>
							<b> Paced-Curriculum Distillation <br> with Prediction and Label Uncertainty <br> for Image Segmentation</b> <br>
							Mobarakol Islam*, <b>Lalithkumar Seenivasan*</b>, SP Sharan, <br> VK Viekash, Bhavesh Gupta, Ben Glocker and Hongliang Ren. <br>
							<em>International Journal of Computer Assisted <br> Radiology and Surgery</em> (<i><b>IJCARS</b></i>), 2023.
							<br> 
							[<a href="https://arxiv.org/pdf/2302.01049.pdf" target="_blank">preprint</a>]
							[<a href="https://github.com/mobarakol/P-CD" target="_blank">code</a>]
						</p>
					</div>
				</div> -->
			</div>
			
			<div id="dataset" class="subheading" height="200px">
				<h2><img src="./pic/dataset.png" class="hicon" style="border-radius: 10px;"/> <u>Dataset Release</u></h2>
				
				<div class="datasettiles datasettile">
					<h2>PSIAVA-VQA</h2>
					<h3><a href="https://drive.google.com/drive/folders/1hu_yK27Xz2_lvjjZ97-WF2MK_JO14MWI" target="_blank">Dataset</a></h3>
					<p>
						Will be Published in MICCAI2023 <br>
						<a href="https://arxiv.org/abs/2304.09974" target="_blank">
							SurgicalGPT: End-to-End Language-Vision <br> 
							GPT for Visual Question Answering in Surgery</a>
						<br>
					</p>
				</div>
				
				<div class="datasettiles datasettile">
					<h2>Cholec80-VQA</h2>
					<h3><a href="https://drive.google.com/drive/folders/1hu_yK27Xz2_lvjjZ97-WF2MK_JO14MWI" target="_blank">Dataset</a></h3>
					<p>
						Published in MICCAI2022 <br>
						<a href="https://link.springer.com/chapter/10.1007/978-3-031-16449-1_4" target="_blank">
							Surgical-VQA: Visual Question Answering <br>
							in Surgical Scenes Using Transformer</a>
						<br>
					</p>
				</div>

				<div class="datasettiles datasettile">
					<h2>EndoVis18-VQA</h2>
					<h3><a href="https://drive.google.com/file/d/16G_Pf4E9KjVq7j_7BfBKHg0NyQQ0oTxP" target="_blank">Dataset</a></h3>
					<p>
						Published in MICCAI2020 paper <br>
						<a href="https://link.springer.com/chapter/10.1007/978-3-030-59716-0_60" target="_blank">
						Learning and Reasoning with the Graph <br>
						Structure Representation in Robotic Surgery</a> 
					</p>
				</div>

				<div class="datasettiles datasettile">
					<h2>EndoVis18-Scene Graph</h2>
					<h3><a href="https://drive.google.com/file/d/16G_Pf4E9KjVq7j_7BfBKHg0NyQQ0oTxP" target="_blank">Dataset</a></h3>
					<p>
						Published in MICCAI2020 paper <br>
						<a href="https://link.springer.com/chapter/10.1007/978-3-030-59716-0_60" target="_blank">
						Learning and Reasoning with the Graph <br>
						Structure Representation in Robotic Surgery</a> 
					</p>
				</div>
			</div>	
			
			<div id="awards" class="subheading">
				<h2><img src="./pic/awards.png" class="hicon"/> <u>Academic Awards</u></h2>

				<div class="awardtiles awardtile">
					<img src="pic/award_ribbon.png" class="award-title"/>
					<br>
					<div id="award-tile-caption">
						<h2> <br> Outstanding Reviewer <br> Award <br>  </h2>
						<h3> International Conference on Medical <br> Image Computing and Computer <br> Assisted Intervention 2023 </h3>
					</div>
				</div>
				
				<div class="awardtiles awardtile">
					<img src="pic/award_ribbon.png" class="award-title"/>
					<br>
					<div id="award-tile-caption">
						<h2> <br> STudent-Author Registration  <br> (STAR) Awards <br>  </h2>
						<h3> International Conference on Medical <br> Image Computing and Computer <br> Assisted Intervention 2023 </h3>
					</div>
				</div>
				

				<div class="awardtiles awardtile">
					<img src="pic/award_ribbon.png" class="award-title"/>
					<br>
					<div id="award-tile-caption">
						<h2> <br> Outstanding Undergraduate <br> Researcher Prize <br>  <br> </h2>
						<h3>National University of Singapore<br>AY2017/2018</h3>
					</div>
				</div>

				<div class="awardtiles awardtile">
					<img src="pic/award_ribbon.png" class="award-title"/>
					<br>
					<div id="award-tile-caption">
						<h2> <br> FoE 32nd INNOVATION <br> & RESEARCH AWARD <br>  <br> </h2>
						<h3>National University of Singapore <br> 2018</h3>
					</div>
				</div>

				<div class="awardtiles awardtile">
					<img src="pic/award_ribbon.png" class="award-title"/>
					<br>
					<div id="award-tile-caption">
						<h2> Singapore Manufacturing <br> Federation Metal, <br> Machinery & Engineering <br> Industry Group Project Prize </h2> 
						<h3> Temasek Polytechnic <br> 2013</h3>
					</div>
				</div>

				<div class="awardtiles awardtile">
					<img src="pic/award_ribbon.png" class="award-title"/>
					<br>
					<div id="award-tile-caption">
						<h2> <br> Commendation Award <br> for Major Project <br> <br></h2>
						<h3> Temasek Polytechnic <br> 2013</h3>
					</div>
				</div>
			</div>

			<div id="talks" class="subheading">

				<h2><img src="./pic/talks.png" class="hicon"/> <u>Key Talks and Presentations</u></h2>
				<div class="talktiles talktile">
					<img src="talk_slide_pic/talk_E2E_icra23.jpg" class="talk-title"/>
					<br>
					<div id="talk-tile-caption">
						<h3> Gradient-Based Localized Feature <br> Extraction for End-To-End <br> Surgical Downstream Tasks </h3>
						<p>
							IEEE International Conference on Robotics <br> and Automation, London, May. 2023.  [<a href="https://youtu.be/wPvV8IvS2tE" target="_blank">video</a>]
						</p>
					</div>
				</div>
				<div class="talktiles talktile">
					<img src="talk_slide_pic/talk_rr_vqa_icra23_ws.jpg" class="talk-title"/>
					<br>
					<div id="talk-tile-caption">
						<h3> Relational Reasoning in VQA Models <br> for Visual Question Answering <br> in Robotic Surgery</h3>
						<p>
							ICRA 2023 Workshop:Surgical Robotics <br> London, Msy. 2023.
						</p>
					</div>
				</div>
				<div class="talktiles talktile">
					<img src="talk_slide_pic/talk_surgical_vqa.jpg" class="talk-title"/>
					<br>
					<div id="talk-tile-caption">
						<h3> Surgical-VQA: Visual Question Answering <br> in Surgical Scenes Using Transformer</h3>
						<p>
							Medical Image Computing and Computer-<br>Assisted Intervention (MICCAI) <br> Singapore, Sep. 2022. [<a href="https://youtu.be/4Db8NSEW-FY" target="_blank">video</a>]
						</p>
					</div>
				</div>
				<div class="talktiles talktile">
					<img src="talk_slide_pic/talk_clearness.jpg" class="talk-title"/>
					<br>
					<div id="talk-tile-caption">
						<h3>CLEARNESS: Cross-scaLe tEmporAl <br> gRaph NEtwork for Super-reSolutions</h3>
						<p>
							IUPESM WORLD CONGRESS ON MEDICAL <br>PHYSICS AND BIOMEDICAL ENGINEERING <br> Singapore, Jun. 2022.
						</p>
					</div>
				</div>
				<div class="talktiles talktile">
					<img src="talk_slide_pic/talk_global_ssu.jpg" class="talk-title"/>
					<br>
					<div id="talk-tile-caption">
						<h3>Global-Reasoned Multi-Task Learning <br> Model for Surgical Scene Understanding</h3>
						<p>
							IEEE International Conference on <br> Robotics and Automation <br> Philadelphia, May. 2022.  [<a href="https://youtu.be/UOIcp3y4o1U" target="_blank">video</a>]
						</p>
					</div>
				</div>
			</div>

			<div id="academic-services" class="subheading">
				<h2><img src="./pic/services.png" class="hicon"> <u>Academic Services</u></h2>
				
				<h3>Program Committee</h3> </li>
				<p>
					<a href="https://sites.google.com/view/dart2022/home" target="_blank"> DART Workshop, Medical Image Computing and Computer Assisted Intervention (MICCAI), 2022. </a></li>
				</p>

				<h3>Reviewer</h3>
				<p>
					Medical Image Computing and Computer Assisted Intervention (MICCAI 2023)<br>
					IEEE Transactions on Medical Imaging (2023)<br>
					IEEE International Conference on Robotics and Automation (2023)<br>
					DART Workshop, Medical Image Computing and Computer Assisted Intervention (2022)<br>
					IEEE Sensors  Journal (2022)
				</p>
					
			</div>

			<div id="footer" class="subheading">	
				<script type='text/javascript' id='clustrmaps' src='//cdn.clustrmaps.com/map_v2.js?cl=ffffff&w=400&t=n&d=9M3ueKzaQxglq5AGb6vfsCiCDSIwT_1Zt8bGvgRFK3I'></script>
				<h3> &copy; Seenivasan Lalithkumar | Last updated: Feb 2024 </h3>
			</div>
		</div>
				
		<div id="Publications" class="tabscontent">
			<div id="publication-charts">
				<div id="conference_chart" class="publication-chart"></div>
				<div id="journal_chart" class="publication-chart"></div>
				<script src="https://canvasjs.com/assets/script/canvasjs.min.js"></script>
			</div>
			
			<div id="conf-journal-publication" class="subheading">
				<h2>	<img src="./pic/manuscript.png" class="hicon"> <u>Conference and Journal Publications</u></h2>

				<div id="conf-journal-publication-list">

					<div class="publicationstiles publicationstile">
						<table>
							<tr>
								<td>
									<img src="./paper_photos/SurgicalGPT.png" class = "publications-img-tile">
								</td>
								<td class ="publications-caption"> 
									<h3>SurgicalGPT: End-to-End <br> Language-Vision GPT for Visual  <br> Question Answering in Surgery. </h3>
									<b>Lalithkumar Seenivasan*</b>, Mobarakol Islam*, <br> Gokul Kannan <br> and Hongliang Ren. <br> 
									<em>Medical Image Computing and Computer- <br> Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2023. <br>
								</td>
							</tr>
						</table>
						<br>
						[<a href="https://arxiv.org/abs/2304.09974" target="_blank">preprint</a>]
						[<a href="https://github.com/lalithjets/" target="_blank">code (will be made available soon)</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<tr>
								<td>
									<img src="./paper_photos/Surgical_VQLA.png" class = "publications-img-tile">
								</td>
								<td class ="publications-caption"> 
									<h3> Surgical-VQLA: Transformer with Gated <br> Vision-Language Embedding for <br> Visual Question Localized-Answering <br> in Robotic Surgery. </h3>
									Long Bai*, Mobarakol Islam*, <br> <b>Lalithkumar Seenivasan</b> and Hongliang Ren. <br> 
									<em>IEEE International Conference on Robotics <br> and Automation </em> (<i><b>ICRA</b></i>), 2023. <br>
								</td>
							</tr>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2305.11692v1.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/longbai1006/surgical-vqla" target="_blank">code</a>]
					</div>
					
					<div class="publicationstiles publicationstile">
						<table>
							<tr>
								<td>
									<img src="./paper_photos/pcd.png" class = "publications-img-tile">
								</td>
								<td class ="publications-caption"> 
									<h3> Paced-Curriculum Distillation with <br> Prediction and Label Uncertainty for <br> Image Segmentation. </h3>
									Mobarakol Islam*, <b>Lalithkumar Seenivasan*</b>, <br> SP Sharan, VK Viekash, Bhavesh Gupta, <br> Ben Glocker and Hongliang Ren. <br>
									<em>International Journal of Computer Assisted <br> Radiology and Surgery</em> (<i><b>IJCARS</b></i>), 2023. <br>
								</td>
							</tr>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2302.01049.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/mobarakol/P-CD" target="_blank">code</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="./paper_photos/TA_MTL.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3>Task-aware asynchronous multi-task <br> model with class incremental contrastive <br> learning for surgical scene understanding. </h3>
								<b>Lalithkumar Seenivasan</b>, Mobarakol Islam, <br> Mengya Xu, Chwee Ming Lim <br> and Hongliang Ren. <br>
								<em>International Journal of Computer Assisted <br> Radiology and Surgery</em> (<i><b>IJCARS</b></i>), 2023. <br>
							</td>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2211.15327.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/lalithjets/Domain-adaptation-in-MTL" target="_blank">code</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="./paper_photos/Rethinking Feature Extraction.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3>Rethinking Feature Extraction: Gradient- <br> Based Localized Feature Extraction for <br> End-To-End Surgical Downstream Tasks. </h3>
								Winnie Pang*, Mobarakol Islam*, Sai Mitheran, <br> <b>Lalithkumar Seenivasan</b>, Mengya Xu and Hongliang Ren. <br>
								<em>IEEE Robotics and Automation Letters & IEEE <br> International Conference on Robotics and <br> Automation</em> (<i><b>RA-L & ICRA</b></i>), 2023.<br>
								
							</td>
						</table>
						<br>
						[<a href="https://discovery.ucl.ac.uk/id/eprint/10159683/1/Rethinking_Feature_Extraction_Gradient-based_Localized_Feature_Extraction_for_End-to-End_Surgical_Downstream_Tasks.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/PangWinnie0219/GradCAMDownstreamTask" target="_blank">code</a>]
						[<a href="https://youtu.be/wPvV8IvS2tE" target="_blank">video</a>] 
					</div>
					
					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="./paper_photos/Surgical_VQA.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Surgical-VQA: Visual Question <br> Answering in Surgical Scenes <br> Using Transformer. </h3>
								<b>Lalithkumar Seenivasan*</b>, Mobarakol Islam*, <br> Adithya K. Krishna <br> and Hongliang Ren. <br>
								<em>Medical Image Computing and Computer- <br> Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2022. <br>
								</td>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2206.11053.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/lalithjets/Surgical_VQA" target="_blank">code</a>]
						[<a href="https://drive.google.com/file/d/19RYYjkokXWv5j_Wayjkc6bEKjSXdzaNB/view?usp=sharing" target="_blank">poster</a>] 
						[<a href="https://youtu.be/4Db8NSEW-FY" target="_blank">video</a>] 
							
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/biomimetic.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Biomimetic Incremental Domain <br> Generalization with a Graph Network <br> for Surgical Scene Understanding. </h3>
								<b>Lalithkumar Seenivasan</b>, Mobarakol Islam,  <br> Chi-Fai Ng, Chwee Ming Lim <br> and Hongliang Ren. <br>
								<em>MDPI Biomimetics Journal</em> <br> (<i><b>Biomimetics</b></i>), 2022.  <br>
							</td>
						</table>
						<br>
						[<a href="https://www.mdpi.com/2313-7673/7/2/68" target="_blank">preprint</a>]
						[<a href="https://github.com/lalithjets/Domain-Generalization-for-Surgical-Scene-Graph" target="_blank">code</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="./paper_photos/Global-Reasoned_Multi-Task_Learning.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Global-Reasoned Multi-Task <br> Learning Model for <br> Surgical Scene Understanding. </h3>
								<b>Lalithkumar Seenivasan*</b>, Sai Mitheran*, <br> Mobarakol Islam and Hongliang Ren. <br>
								<em>IEEE Robotics and Automation Letters & IEEE <br> International Conference on Robotics and  <br> Automation</em> (<i><b>RA-L & ICRA</b></i>), 2022. <br>
							</td>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2201.11957.pdf" target="_blank">preprint</a>][<a href="https://github.com/lalithjets/Global-reasoned-multi-task-model" target="_blank">code</a>]
						[<a href="https://drive.google.com/file/d/1deAY5ThZRm9Y3AFX3ArIvde-LdE8imfx/view?usp=sharing" target="_blank">poster</a>]
						[<a href="https://youtu.be/UOIcp3y4o1U" target="_blank">video</a>] 
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/PetRAW.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> PEg TRAnsfer Workflow Recognition <br> Challenge Report: Do Multi-Modal <br> Data Improve Recognition?. </h3>
								Arnaud Huaulmé, et. al, ...... <br> <b>Lalithkumar Seenivasan</b> and Pierre Jannin. <br>
								<em>EndoVis21 Challenge, Medical Image <br> Computing  and Computer Assisted Intervention</em> <br> (<i><b>Challenge, MICCAI</b></i>), 2021.  <br> 
							</td>
						</table>
						<br>
						[<a href="https://papers.ssrn.com/sol3/papers.cfm?abstract_id=4088403" target="_blank">preprint</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/CholecTripLet.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> CholecTriplet2021: A benchmark <br> challenge for surgical action <br> triplet recognition. </h3>
								Chinedu Innocent Nwoye, et. al, ...... <br> <b>Lalithkumar Seenivasan</b> and Nicolas Padoy. <br>
								<em>EndoVis21 Challenge, Medical Image <br> Computing and Computer Assisted Intervention</em> <br> (<i><b>Challenge, MICCAI</b></i>), 2021.  <br> 
							</td>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2204.04746.pdf" target="_blank">preprint</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/cda.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Class-Distribution-Aware <br> Calibration for Long-Tailed <br> Visual Recognition. </h3>
								Mobarakol Islam, <b>Lalithkumar Seenivasan</b>, <br> Hongliang Ren <br> and Ben Glocker. <br>
								<em>UDL Workshop, International Conference on <br> Machine Learning</em> (<i><b>UDL , ICML</b></i>), 2021.  <br> 
							</td>
						</table>
						<br>
						[<a href="https://arxiv.org/pdf/2109.05263.pdf" target="_blank">preprint</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/ScoopNet.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3>ScoopNet: 6DOF Pose Estimation <br> pipeline for Origami-inspired <br> Worm Robots. </h3>
								Rohit Lal, Ruphan Swaminathan, <br> <b>Lalithkumar Seenivasan</b>, Liang Qiu <br> and Hongliang Ren. <br>
								<em>IEEE International Conference on <br> Development and Learning</em> (<i><b>ICDL</b></i>), 2021. 
							</td>
						</table>
						<br> 
						[<a href="https://www.researchgate.net/profile/Liang-Qiu-7/publication/354075045_ScoopNet_6DOF_Pose_Estimation_pipeline_for_Origami-inspired_Worm_Robots/links/62ac796923f3283e3aef5d29/ScoopNet-6DOF-Pose-Estimation-pipeline-for-Origami-inspired-Worm-Robots.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/take2rohit/scoop_net" target="_blank">code</a>]  
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/shape_Tracking.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3>Shape Tracking of Flexible <br> Morphing Matters From <br> Depth Images. </h3> 
								<b>Lalithkumar Seenivasan</b>, Fan Bai, Ming Ji, <br> Xiaoyi Gu, Zion Tsz Ho Tse <br> and Hongliang Ren. <br>
								<em>IEEE Sensors Journal</em>, 2021.  <br> 
							</td>
						</table>
						<br>
						[<a href="https://ieeexplore.ieee.org/abstract/document/9262941/" target="_blank">preprint</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/StentNet.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Stent Deployment Detection Using <br> Radio Frequency-Based Sensor <br> and Convolutional Neural Networks. </h3>
								Mengya Xu*, <b>Lalithkumar Seenivasan*</b>, <br> Leonard Leong Litt Yeo <br> and Hongliang Ren. <br>
								<em>Advanced Intelligent Systems</em>  <br> (<i> <b>Adv. Intell. Syst.</b></i>), 2020.  
							</td>
						</table>
						<br> 
						[<a href="https://onlinelibrary.wiley.com/doi/pdfdirect/10.1002/aisy.202000092" target="_blank">preprint</a>]
						[<a href="https://github.com/XuMengyaAmy/StentDeploymentDetectionWithStentNet" target="_blank">code</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/Learning_Reasoning_Surgical_Graph.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Learning and Reasoning with <br> the Graph Structure Representation <br> in Robotic Surgery. </h3>
								Mobarakol Islam, <b>Lalithkumar Seenivasan</b>, <br> Lim Chwee Ming and Hongliang Ren. <br>
								<em>Medical Image Computing and Computer <br> Assisted Intervention</em> (<i><b>MICCAI</b></i>), 2020.
							</td>
						</table>
						<br> 
						[<a href="https://arxiv.org/pdf/2007.03357.pdf" target="_blank">preprint</a>]
						[<a href="https://github.com/mobarakol/Surgical_SceneGraph_Generation" target="_blank">code</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<td>
								<img src="paper_photos/Pilot_study.png" class = "publications-img-tile">
							</td>
							<td class ="publications-caption"> 
								<h3> Pilot Study and Design Conceptualization for <br> a Slim Single-port Surgical Manipulator with <br> Spring Backbones and Catheter-size Channels. </h3>
								Hongliang Ren, Cai Xin Chen, Catherine Cai, <br> Krishna Ramachandra <br> and <b>Lalithkumar Seenivasan</b>.<br>
								<em>IEEE International Conference on <br> Information and Automation</em> (<i><b>ICIA</b></i>), 2017.
							</td>
						</table>
						<br> 
						[<a href="https://d1wqtxts1xzle7.cloudfront.net/60791984/IEEE_EG330120191003-28729-olwc41-libre.pdf?1570169551=&response-content-disposition=inline%3B+filename%3DPilot_Study_and_Design_Conceptualization.pdf&Expires=1676488988&Signature=aRqZ0eQ86e90tOu0xDDuaXswjMy6-G4oaWJrSObIsjEsArlEMbkqAyKyGy7jy5bjhdenfZ03zX3levTPYb7glvkoyuvT~XUxGOfObOc0Jn3nQ6UHW8LOqmeRNUXKZSkZToqS5Ek3DNHjkyMv66ge7SEYetnN0IW7POaNTM1-Flk9PERABzYKMUy6jHOvhJRBNzW9wisbgkdUWhigJ7PNiG3S0zP4XgPd268~keozPXJGo5mkSeI3A-zMGd3eHiTJzdaxoU4EmahLsEY-T-He7X~DACt~vcCxiYj5EJ54AsXKk9SIOL9JJmIb2fXJ9VBkpMhIefLbvq2uNb21Fv4LRA__&Key-Pair-Id=APKAJLOHF5GGSLRBV4ZA"
							target="_blank">preprint</a>]
					</div>
				</div>
			</div>

			<div id="book-publication" class="subheading">
				<h2>	<img src="./pic/book.png" class="hicon" style="border-radius: 10px;"/> <u>Book Publications</u></h2>
	
				<div id="book-publication-list">
					<div class="publicationstiles publicationstile">
						<table>
							<tr>
								<td>
									<img src="./paper_photos/untethered_soft_ferromagnetic.png" class = "publications-img-tile">
								</td>
								<td class ="publications-caption"> 
									<h3> Untethered Soft Ferromagnetic <br> Quad-Jaws Cootie Catcher with <br> Selectively Coupled Degrees of Freedom. </h3>
									Xinchen Cai, Catherine Jiayi Cai, <br> <b>Lalithkumar Seenivasan</b>, Zion Tse <br>  and Hongliang Ren. <br>
									<em>Deployable Multimodal Machine Intelligence: <br> </b>Applications in Biomedical Engineering</em>, <br> <i><b>Springer Nature Singapore</b></i>, 2023.
								</td>
							</tr>
						</table>
						<br>
						[<a href="https://link.springer.com/chapter/10.1007/978-981-19-5932-5_13" target="_blank">preprint</a>] 
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<tr>
								<td>
									<img src="./paper_photos/tendon_routing.png" class = "publications-img-tile">
								</td>
								<td class ="publications-caption"> 
									<h3>Tendon routing and anchoring for cable- <br>driven single-port surgical manipulators with <br> spring backbones and luminal constraints. </h3>
									<b>Lalithkumar Seenivasan</b>, Xinchen Cai, <br> Krishna Ramachandra, Francis Wong <br> and Hongliang Ren. <br>
									<em>Flexible Robotics in Medicine: A Design Journey of  <br>Motion Generation Mechanisms and Biorobotic <br> System Development</em>, <i><b>Academic Press</b></i>, 2020.
								</td>
							</tr>
						</table>
						<br>
						[<a href="https://books.google.com.sg/books?hl=en&lr=&id=1WnnDwAAQBAJ&oi=fnd&pg=PA169&dq=info:S14J972tDtcJ:scholar.google.com&ots=L-9uPEaSLm&sig=UN07TG90nE0A6uLshB_Ram-BgRw&redir_esc=y#v=onepage&q&f=false" target="_blank">preprint</a>]
					</div>

					<div class="publicationstiles publicationstile">
						<table>
							<tr>
								<td>
									<img src="./paper_photos/tunable_stiffness.png" class = "publications-img-tile">
								</td>
								<td class ="publications-caption"> 
									<h3> Tunable stiffness using negative Poisson's <br> ratio toward load-bearing continuum tubular <br> mechanisms in medical robotics. </h3>
									Krishna Ramachandra, Catherine Jiayi Cai, <br> <b>Lalithkumar Seenivasan</b>, Xinchen Cai, <br> Zion Tszho Tse and Hongliang Ren. <br>
									<em>Control Theory in Biomedical Engineering</em>, <br> <i><b>Academic Press</b></i>, 2020.
								</td>
							</tr>
						</table>
						<br>
						[<a href="https://www.sciencedirect.com/science/article/pii/B9780128213506000123" target="_blank">preprint</a>]
					</div>
				</div>
			</div>
					
			<div id="footer" class="subheading">	
				<h3> &copy; Seenivasan Lalithkumar | Last updated: July 2023 </h3>
			</div>
		</div>
		<div id="Robotic-Projects" class="tabscontent">
			<div id="BoaIV-2017" class="subheading">
				<h1> Undergraduate Projects </h1>
				<h2> Boa-IV: Image-Guided Minimally Invasive Robotic Surgery </h2>
				<br>
				<p>
					Our team conceptualized and designed a novel tendon-driven actuation mechanism for stem actuation of catheter-size spring-backbone manipulators.
					Using this novel concept, we are also able to miniaturize surgical tools to a diameter of 2-3m that can be manipulated to perform surgical actions. 
					The miniaturized surgical tools can be deployed through a single-port flexible manipulator, which has an overall shaft diameter of 8-12 mm.
				</p>
				<div class="boaiv-tiles ">
					<img src='robotic_projects/BoaIV-I3.jpg' class="boaiv-image"/>
					<br>
					<div id="boaiv-tile-caption">
						<h3>Minimum Viable Product</h3>
					</div>
				</div>
				<br>
				<div id="BoaIV-video">
					<img src='robotic_projects/BOAIVV1.gif' class="videogif"/>
					<img src='robotic_projects/BOAIVV2.gif' class="videogif"/>
					<br>
					<h3> Slave (Forcepts and electrocautery) Control Demo</h3>
				</div>
				<br>
				<h2>Related Papers </h2>
				<h3 style="margin-bottom: 5px;"> Pilot Study and Design Conceptualization for a Slim Single-port Surgical Manipulator with Spring Backbones and Catheter-size Channels. </h3>
				<p style="text-align: center;">
					Hongliang Ren, Cai Xin Chen, Catherine Cai, Krishna Ramachandra and <b>Lalithkumar Seenivasan</b>.<br>
					<em>IEEE International Conference on Information and Automation</em> (<i><b>ICIA</b></i>), 2017.
				</p>
				<h3 style="margin-bottom: 5px;">Tendon routing and anchoring for cable-driven single-port surgical manipulators with spring backbones and luminal constraints. </h3>
				<p style="text-align: center;">
					<b>Lalithkumar Seenivasan</b>, Xinchen Cai, Krishna Ramachandra, Francis Wong and Hongliang Ren. <br>
					<em>Flexible Robotics in Medicine: A Design Journey of Motion Generation Mechanisms and Biorobotic System Development</em>, <i><b>Academic Press</b></i>, 2020.
				</p>
				<br>
			</div>
			<div id="techXChallenge-2013" class="subheading">
				<h1> Diploma Projects </h1>
				<h2> TechX Challenge 2013, Singapore</h2>
				<br>
				<p>
					I was part of the <em> Temasek Polytechnic team </em>in the <em>TechX 2013 National Robotic Competition </em> organized by the <em>Defence Science & Technology 
					Agency (DSTA), Singapore </em>. The competition comprised 6 challenges, where, the robots were assessed on their ability to autonomously navigate 
					challenging terrains to conduct search and rescue missions and simultaneously relay information to base station in real-time.  Our team managed 
					to pass the qualification round and qualified for the grand finals. 
				</p>
				<div id="techxRobot">
					<img src='robotic_projects/techXchallengeR1.gif' class="videogif"/>
					<img src='robotic_projects/techXchallengeR2.gif' class="videogif"/>
				</div>
				<br>
				<div id="techxspeccontribution">
					<h3> My Specific Contribution </h3>
					<br>
					<p>
						I designed and developed 3D simulations of our robot and competition challenge 
						environments using the ROS and Gazebo simulator. The resultant simulations enabled 
						the software team to test our navigation and sensor processing algorithms.
					</p>
					<div id="robot-env-contribution">
						<table >
							<tr>
								<td class ="TechXTable">
									<h3> Robot Simulation</h3>
									<p>
										<ul>
											<li> Robot body</li>
											<li> Robot Joint Control: Simplify track system to suit ROS-Gazebo Simulation </li>
											<li> Lidar and vision sensor simulation</li>
											<li> GPS simulation</li>
										</ul>
									</p>
								</td>
								<td class ="TechXTable"> 
									<h3> Challenge Simulation</h3>
									<p>
										<ul>
											<li> Uneven terrain</li>
											<li> Challenge specific enviroment: Barriers, dynamic obstacles, uneven terrians, staircases and target objects</li>
											<li> Urban enviroment: with flat terrains, uneven terrains and multi-story houses</li>
										</ul>
									</p>
								</td>
							</tr>
						</table>
					</div>
				</div>
				<br>
				<div id="techx-simulations">
					<div class="techx-simtiles techx-simtile">
						<img src='robotic_projects/techXchallengeE1.gif' class="videogif"/>
						<br>
						<div id="techx-sim-tile-caption">
							<h3>Robot and Challenge Simulation</h3>
						</div>
					</div>
					<div class="techx-simtiles techx-simtile">
						<img src='robotic_projects/techXchallengeE2.gif' class="videogif"/>
						<br>
						<div id="techx-sim-tile-caption">
							<h3>Obstacle Avoidance</h3>
						</div>
					</div>
					<div class="techx-simtiles techx-simtile">
						<img src='robotic_projects/techXchallengeE3.gif' class="videogif"/>
						<br>
						<div id="techx-sim-tile-caption">
							<h3>Staircase Climbing</h3>
						</div>
					</div>
					<div class="techx-simtiles techx-simtile">
						<img src='robotic_projects/techXchallengeE4.gif' class="videogif"/>
						<br>
						<div id="techx-sim-tile-caption">
							<h3>Pan Scan</h3>
						</div>
					</div>
				</div>
			</div>
			<div id="footer" class="subheading">	
				<h3> &copy; Seenivasan Lalithkumar | Last updated: Feb 2024 </h3>
			</div>
		</div>
		<div id="Resume" class="tabscontent">
			<div id="resume-content" class="subheading">
				<h2><img src="./pic/biography.png" class="hicon"> <u>Resume</u> </h2>
				<a href="https://github.com/lalithjets/lalithjets.github.io/blob/master/resume/Seenivasan_lalithkumar_resume.pdf"> (<u>Full PDF</u>)</a>
				<div style="height: 2300px;">
					<iframe src="https://docs.google.com/gview?url=https://github.com/lalithjets/lalithjets.github.io/raw/master/resume/Seenivasan_lalithkumar_resume.pdf&embedded=true" style="width:100%; height:100%;" frameborder="0"></iframe>	
				</div>
				<div id="footer" class="subheading">	
					<h3> &copy; Seenivasan Lalithkumar | Last updated: July 2023 </h3>
				</div>
			</div>			
		</div>
		<script src="https://canvasjs.com/assets/script/canvasjs.min.js"></script>
		<script src="opentab.js" type="text/javascript"></script>
	</body>

</html>
